{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to: Anomaly detection in nanoseconds on an FPGA\n",
    "\n",
    "<img src=\"images/front.png\" alt=\"The ADC2021 Challenge\" width=\"300\" img align=\"right\"/>\n",
    "\n",
    "In this notebook we will demonstrate how to design a tiny autoencoder (AE) that we will use for anomaly detection in particle physics. More specifically, we will demonstrate how we can use autoencoders to select potentially New Physics enhanced proton collision events in a more unbiased way than with the usual Level-1 trigger algorithms!\n",
    "\n",
    "Some of the key tools we will use in order to make our AE fast and small enought to fit within the strict latency and resource budget of a L1 trigger algorithm are:\n",
    "- Quantization\n",
    "- Pruning\n",
    "- Highly parallel deployment using hls4ml!\n",
    "\n",
    "We will train the autoencoder to learn to compress and decompress data, assuming that for highly anomalous events, the AE will fail.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "As a dataset, we will use the [ADC2021 dataset](https://mpp-hep.github.io/ADC2021/). It is represented as an array of up to 4 e/ùõæ, missing transverse energy (MET), 4 Œº and 10 jets each described by pT, Œ∑ and œÜ to mimic a L1 data format (a total of 57 inputs per event). The particles are ordered by pT. \n",
    "\n",
    "You can train using the provided 4 million background-like events \n",
    "simulated with Delphes, where the events are pre-filtered to have at least one lepton\n",
    "<img src=\"images/datagrid.png\" alt=\"Background data\" width=\"300\" img align=\"right\"/>\n",
    "- Inclusive W production, with W ‚Üí lùúà (59.2%)\n",
    "- Inclusive Z production, with Z ‚Üí ll (6.7%)\n",
    "- tt production (0.3%)\n",
    "- QCD multijet production (33.8%)\n",
    "\n",
    "You can then evaluate the AE performance on several different New Physics simulated samples: \n",
    "- Neutral scalar boson A, 50 GeV ‚Üí 4 l \n",
    "- Leptoquark, 80 GeV ‚Üí b œÑ \n",
    "- Scalar boson, 60 GeV ‚Üí œÑ œÑ \n",
    "- Charged scalar boson, 60 GeV ‚Üí œÑ ùúà \n",
    "- Black Box (mix of background and an unknown signal!!)\n",
    "\n",
    "We'll train using all the background data and test using the A (50 GeV) ‚Üí 4 l sample. Let's fetch them! The background data can be downloaded [here](https://zenodo.org/record/5046389#.YaeRWL3MLze) and the signal data [here](https://zenodo.org/record/5046446#.YaeSa73MLzd). I have already downloaded it and moved it to folder called `data/`.\n",
    "\n",
    "Let's prepare the data! For simplicity, we'll only use a million of the background events. We also flatten the 2D grid into a 1D array to prepare feeding it into a dense network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "nbkg_events = 1000000\n",
    "# read BACKGROUND data and shuffle it in preparation for training. This takes a while so I've done it in advance!\n",
    "with h5py.File('data/background_for_training.h5', 'r') as file:\n",
    "    full_data = file['Particles'][:,:,:-1]\n",
    "    print(\"Original data shape = (N samples, N particles, N features) = \",full_data.shape)\n",
    "    np.random.shuffle(full_data)\n",
    "    if nbkg_events: full_data = full_data[:nbkg_events,:,:]\n",
    "    \n",
    "        \n",
    "# define training, test and validation datasets\n",
    "X_train, X_test = train_test_split(full_data, test_size=0.2, shuffle=True)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2)\n",
    "\n",
    "del full_data\n",
    "\n",
    "input_shape= X_train.shape[1]*X_train.shape[2]\n",
    "# flatten the data for model input\n",
    "X_train = X_train.reshape(X_train.shape[0], input_shape)\n",
    "X_test = X_test.reshape(X_test.shape[0], input_shape)\n",
    "X_val = X_val.reshape(X_val.shape[0], input_shape)\n",
    "print(\"Training data shape = \",X_train.shape)    \n",
    "with h5py.File('bkg_dataset.h5', 'w') as h5f:\n",
    "    h5f.create_dataset('X_train', data = X_train)\n",
    "    h5f.create_dataset('X_test', data = X_test)\n",
    "    h5f.create_dataset('X_val', data = X_val)\n",
    "    \n",
    "with h5py.File('data/Ato4l_lepFilter_13TeV.h5', 'r') as file:\n",
    "    signal_data = file['Particles'][:,:,:-1]\n",
    "    signal_data = signal_data.reshape(signal_data.shape[0],input_shape)\n",
    "with h5py.File('Ato4l_dataset.h5', 'w') as h5f2:\n",
    "    h5f2.create_dataset('Data', data = signal_data)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You  now have two new files in your reposity, `bkg_dataset.h5` and `Ato4l_dataset.h5` which contains your train/test/val data to train the autoencoder, as well as a test data to check your performance on a New Physics signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir('.')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('bkg_dataset.h5', 'r') as file:\n",
    "    X_train = np.array(file['X_train'])\n",
    "    X_test = np.array(file['X_test'])\n",
    "    X_val = np.array(file['X_val'])\n",
    "    \n",
    "with h5py.File('Ato4l_dataset.h5', 'r') as file:\n",
    "    signal_test_data = np.array(file['Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\" Training (#samples,#features):\", X_train.shape)\n",
    "print(\" Testing  (#samples,#features):\", signal_test_data.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(8,5))\n",
    "fig.suptitle('Kinematic distributions in bkg vs signal')\n",
    "\n",
    "axs[0].hist(X_train[:,0],bins=100,label=r'Background',histtype='step', linewidth=2, facecolor='none', edgecolor='green',fill=True,density=True)\n",
    "axs[0].hist(signal_test_data[:,0],bins=100,label=r'Signal',histtype='step', linewidth=2, facecolor='none', edgecolor='orchid',fill=True,density=True)\n",
    "axs[0].semilogy()\n",
    "axs[0].set(xlabel=u'Leading electron $p_{T}$ (GeV)', ylabel='A.U')\n",
    "axs[0].legend(loc='best',frameon=False, ncol=1,fontsize='large')\n",
    "\n",
    "axs[1].hist(X_train[:,15],bins=100,label=r'Background',histtype='step', linewidth=2, facecolor='none', edgecolor='green',fill=True,density=True)\n",
    "axs[1].hist(signal_test_data[:,15],bins=100,label=r'Signal',histtype='step', linewidth=2, facecolor='none', edgecolor='orchid',fill=True,density=True)\n",
    "axs[1].set(xlabel=u'Leading muon $p_{T}$ (GeV)', ylabel='A.U')\n",
    "axs[1].semilogy()\n",
    "axs[1].legend(loc='best',frameon=False, ncol=1,fontsize='large')\n",
    "print(signal_test_data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the autoencoder\n",
    "\n",
    "Now, let's define an autoencoder to learn to reconstruct the training data after compressing it through a bottleneck, then decompressing it again.\n",
    "\n",
    "<img src=\"images/ae.png\" alt=\"The autoencoder\" width=\"800\" img align=\"center\"/>\n",
    "\n",
    "For that, we need a stack of dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "input_shape = 57\n",
    "latent_dim = 3\n",
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "x = BatchNormalization()(inputArray)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "#decoder\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "#create autoencoder\n",
    "autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "\n",
    "callbacks=[]\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "if train:\n",
    "    history = autoencoder.fit(X_train, X_train, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_data=(X_val, X_val),\n",
    "                  callbacks=callbacks)\n",
    "    # Save the model\n",
    "    autoencoder.save('baseline_ae.h5')\n",
    "    \n",
    "else:\n",
    "    autoencoder = tf.keras.models.load_model('baseline_ae.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model performance\n",
    "\n",
    "Remember that the key metric we use for anomaly detection is the mean-squared-error: If the error is high, the data is more likely to be anomalous, and if the error is low, the data is similar to the training data (which in our case is SM events). We therefore first need to run `model.predict()` in order to get the AE reconstructed output, both for our vanilla SM test data, and for our new leptoquark signal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_prediction = autoencoder.predict(X_test)\n",
    "signal_prediction = autoencoder.predict(signal_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to compute the mean-square-error, which will be our final discriminating variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(true, prediction):\n",
    "    loss = tf.reduce_mean(tf.math.square(true - prediction),axis=-1)\n",
    "    return loss\n",
    "\n",
    "# compute loss value of input data versus AE reconstructed data\n",
    "mse_sm = mse_loss(X_test, bkg_prediction.astype(np.float32)).numpy()\n",
    "mse_bsm = mse_loss(signal_test_data,signal_prediction.astype(np.float32)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at our discriminant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size=100\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(mse_sm, bins=bin_size, label=\"SM Background\", density = True, histtype='step', fill=False, edgecolor='green', linewidth=1.5)\n",
    "plt.hist(mse_bsm, bins=bin_size, label=\"Leptoquark\", density = True, histtype='step', fill=False, edgecolor='orchid', linewidth=1.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Mean-squared-error\")\n",
    "plt.ylabel(\"Probability (a.u.)\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some discrimination power if we cut at very high values of the MSE! Let's look at a ROC curve to make it easier to vizualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "target_background = np.zeros(mse_sm.shape[0])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "trueVal = np.concatenate((np.ones(mse_bsm.shape[0]), target_background)) # anomaly=1, bkg=0\n",
    "predVal_loss = np.concatenate((mse_bsm, mse_sm))\n",
    "\n",
    "fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    \n",
    "plt.plot(fpr_loss, tpr_loss, \"-\", label='Leptoquark (auc = %.1f%%)'%(auc_loss*100.), linewidth=1.5, color = \"orchid\")\n",
    "    \n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend(loc='center right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='green', linestyle='dashed', linewidth=2) # threshold value for measuring anomaly detection efficiency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! So at a false positive rate of 10E-5, the signal efficiency is almost three orders of magnitude higher! This can obviously be further improved, but I leave that up to you :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model compression\n",
    "\n",
    "Now, there is absolutely no way anyone would let you deploy this model on an FPGA in the trigger. It will use far too many resources! Luckily, as we discussed in the lecture, there are some cheap tricks you can perform to compress the model. These are pruning and quantization-aware-training and both are very easily implemented. Let's have a look.\n",
    "\n",
    "To quantize the model during training, such that the network will get the opportunity to adapt to the narrower bitwidth we use the library [QKeras](https://www.nature.com/articles/s42256-021-00356-5.epdf?sharing_token=A6MQVmmncHNyCtDUXzrqtNRgN0jAjWel9jnR3ZoTv0N3uekY-CrHD1aJ9BTeJNRfQ1EhZ9jJIhgZjfrQxrmxMLMZ4eGzSeru7-ASFE-Xt3NVE6yorlffwUN0muAm1auU2I6-5ug4bOLCRYvA0mp-iT-OdPsrBYeH0IHRYx0t3wc%3D), developed in a joint effort between CERN and Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import QDense, QActivation\n",
    "\n",
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "x = BatchNormalization()(inputArray)\n",
    "x = QDense(32, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "x = QDense(16, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "encoder = QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(x)\n",
    "\n",
    "#decoder\n",
    "x = QDense(16, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(encoder)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "x = QDense(32, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "decoder = QDense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(x)\n",
    "\n",
    "#create autoencoder\n",
    "q_autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "q_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy as that! Let's add some pruning on top, 50% sparsity (removing half of the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
    "model = prune.prune_low_magnitude(q_autoencoder, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss='mse')\n",
    "train = True\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "\n",
    "callbacks=[]\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "if train:\n",
    "    history = q_autoencoder.fit(X_train, X_train, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_data=(X_val, X_val),\n",
    "                  callbacks=callbacks)\n",
    "    # Save the model\n",
    "    q_autoencoder.save('compressed_ae.h5')\n",
    "    \n",
    "else:\n",
    "    q_autoencoder = tf.keras.models.load_model('compressed_ae.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the performance to the floating point precision, unpruned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_prediction = q_autoencoder.predict(X_test)\n",
    "signal_prediction = q_autoencoder.predict(signal_test_data)\n",
    "\n",
    "# compute loss value of input data versus AE reconstructed data\n",
    "q_mse_sm = mse_loss(X_test, bkg_prediction.astype(np.float32)).numpy()\n",
    "q_mse_bsm = mse_loss(signal_test_data,signal_prediction.astype(np.float32)).numpy()\n",
    "\n",
    "target_background = np.zeros(q_mse_sm.shape[0])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "trueVal = np.concatenate((np.ones(q_mse_bsm.shape[0]), target_background)) # anomaly=1, bkg=0\n",
    "predVal_loss = np.concatenate((q_mse_bsm, q_mse_sm))\n",
    "\n",
    "q_fpr_loss, q_tpr_loss, q_threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "q_auc_loss = auc(q_fpr_loss, q_tpr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "    \n",
    "plt.plot(fpr_loss, tpr_loss, \"-\", label='Baseline (auc = %.1f%%)'%(auc_loss*100.), linewidth=1.5, color = \"orchid\")\n",
    "plt.plot(q_fpr_loss, q_tpr_loss, \"-\", label='Quantized+Pruned (auc = %.1f%%)'%(q_auc_loss*100.), linewidth=1.5, color = \"green\")\n",
    "\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend(loc='center right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='orange', linestyle='dashed', linewidth=2) # threshold value for measuring anomaly detection efficiency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUCH! We really took a hit. Well, whatever. Let's tune that another time and nonw try deploying these models!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
