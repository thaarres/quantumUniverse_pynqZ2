{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to: Anomaly detection in nanoseconds on an FPGA\n",
    "\n",
    "<img src=\"images/front.png\" alt=\"The ADC2021 Challenge\" width=\"300\" img align=\"right\"/>\n",
    "\n",
    "In this notebook we will demonstrate how to design a tiny autoencoder (AE) that we will use for anomaly detection in particle physics. More specifically, we will demonstrate how we can use autoencoders to select potentially New Physics enhanced proton collision events in a more unbiased way than with the usual Level-1 trigger algorithms!\n",
    "\n",
    "Some of the key tools we will use in order to make our AE fast and small enought to fit within the strict latency and resource budget of a L1 trigger algorithm are:\n",
    "- Quantization\n",
    "- Pruning\n",
    "- Highly parallel deployment using hls4ml!\n",
    "\n",
    "We will train the autoencoder to learn to compress and decompress data, assuming that for highly anomalous events, the AE will fail.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "As a dataset, we will use the [ADC2021 dataset](https://mpp-hep.github.io/ADC2021/). It is represented as an array of up to 4 e/ùõæ, missing transverse energy (MET), 4 Œº and 10 jets each described by pT, Œ∑ and œÜ to mimic a L1 data format (a total of 57 inputs per event). The particles are ordered by pT. \n",
    "\n",
    "You can train using the provided 4 million background-like events \n",
    "simulated with Delphes, where the events are pre-filtered to have at least one lepton\n",
    "<img src=\"images/datagrid.png\" alt=\"Background data\" width=\"300\" img align=\"right\"/>\n",
    "- Inclusive W production, with W ‚Üí lùúà (59.2%)\n",
    "- Inclusive Z production, with Z ‚Üí ll (6.7%)\n",
    "- tt production (0.3%)\n",
    "- QCD multijet production (33.8%)\n",
    "\n",
    "You can then evaluate the AE performance on several different New Physics simulated samples: \n",
    "- Neutral scalar boson A, 50 GeV ‚Üí 4 l \n",
    "- Leptoquark, 80 GeV ‚Üí b œÑ \n",
    "- Scalar boson, 60 GeV ‚Üí œÑ œÑ \n",
    "- Charged scalar boson, 60 GeV ‚Üí œÑ ùúà \n",
    "- Black Box (mix of background and an unknown signal!!)\n",
    "\n",
    "We'll train using all the background data and test using the A (50 GeV) ‚Üí 4 l sample. Let's fetch them! The background data can be downloaded [here](https://zenodo.org/record/5046389#.YaeRWL3MLze) and the signal data [here](https://zenodo.org/record/5046446#.YaeSa73MLzd). I have already downloaded it and moved it to folder called `data/`.\n",
    "\n",
    "Let's prepare the data! For simplicity, we'll only use a million of the background events. We also flatten the 2D grid into a 1D array to prepare feeding it into a dense network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape = (N samples, N particles, N features) =  (4000000, 19, 3)\n",
      "Training data shape =  (640000, 57)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nbkg_events = 1000000\n",
    "# read BACKGROUND data and shuffle it in preparation for training. This takes a while so I've done it in advance!\n",
    "with h5py.File('data/background_for_training.h5', 'r') as file:\n",
    "    full_data = file['Particles'][:,:,:-1]\n",
    "    print(\"Original data shape = (N samples, N particles, N features) = \",full_data.shape)\n",
    "    np.random.shuffle(full_data)\n",
    "    if nbkg_events: full_data = full_data[:nbkg_events,:,:]\n",
    "        \n",
    "# define training, test and validation datasets\n",
    "X_train, X_test = train_test_split(full_data, test_size=0.2, shuffle=True)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2)\n",
    "\n",
    "del full_data\n",
    "\n",
    "input_shape= X_train.shape[1]*X_train.shape[2]\n",
    "# flatten the data for model input\n",
    "X_train = X_train.reshape(X_train.shape[0], input_shape)\n",
    "X_test = X_test.reshape(X_test.shape[0], input_shape)\n",
    "X_val = X_val.reshape(X_val.shape[0], input_shape)\n",
    "print(\"Training data shape = \",X_train.shape)    \n",
    "with h5py.File('bkg_dataset.h5', 'w') as h5f:\n",
    "    h5f.create_dataset('X_train', data = X_train)\n",
    "    h5f.create_dataset('X_test', data = X_test)\n",
    "    h5f.create_dataset('X_val', data = X_val)\n",
    "    \n",
    "with h5py.File('data/Ato4l_lepFilter_13TeV.h5', 'r') as file:\n",
    "    signal_data = file['Particles'][:,:,:-1]\n",
    "    signal_data = signal_data.reshape(signal_data.shape[0],input_shape)\n",
    "with h5py.File('Ato4l_dataset.h5', 'w') as h5f2:\n",
    "    h5f2.create_dataset('Data', data = signal_data)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You  now have two new files in your reposity, `bkg_dataset.h5` and `Ato4l_dataset.h5` which contains your train/test/val data to train the autoencoder, as well as a test data to check your performance on a New Physics signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.gitignore', 'README.md', 'environment.yml', 'images', 'part1_ADmodel.ipynb', 'part2_hls4ml.ipynb', 'part3_pynqz2.ipynb', 'prepare_adc_data.py', 'util.py', '.ipynb_checkpoints', 'data', 'bkg_dataset.h5', 'Ato4l_dataset.h5', 'baseline_ae.h5', 'compressed_ae.h5', '__pycache__', 'baseline_ae_pynq']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir('.')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('bkg_dataset.h5', 'r') as file:\n",
    "    X_train = np.array(file['X_train'])\n",
    "    X_test = np.array(file['X_test'])\n",
    "    X_val = np.array(file['X_val'])\n",
    "    \n",
    "with h5py.File('Ato4l_dataset.h5', 'r') as file:\n",
    "    signal_test_data = np.array(file['Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training (#samples,#features): (640000, 57)\n",
      " Testing  (#samples,#features): (55969, 57)\n",
      "[18.43733406  0.         -2.66409016 30.53505325  2.31486845  0.68392003\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         18.25468636  1.1769315   1.64127874\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         39.76318359  2.31548548 -2.58200836\n",
      " 30.53505325  2.31486845  0.68392003  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\" Training (#samples,#features):\", X_train.shape)\n",
    "print(\" Testing  (#samples,#features):\", signal_test_data.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(8,5))\n",
    "fig.suptitle('Kinematic distributions in bkg vs signal')\n",
    "\n",
    "axs[0].hist(X_train[:,0],bins=100,label=r'Background',histtype='step', linewidth=2, facecolor='none', edgecolor='green',fill=True,density=True)\n",
    "axs[0].hist(signal_test_data[:,0],bins=100,label=r'Signal',histtype='step', linewidth=2, facecolor='none', edgecolor='orchid',fill=True,density=True)\n",
    "axs[0].semilogy()\n",
    "axs[0].set(xlabel=u'Leading electron $p_{T}$ (GeV)', ylabel='A.U')\n",
    "axs[0].legend(loc='best',frameon=False, ncol=1,fontsize='large')\n",
    "\n",
    "axs[1].hist(X_train[:,15],bins=100,label=r'Background',histtype='step', linewidth=2, facecolor='none', edgecolor='green',fill=True,density=True)\n",
    "axs[1].hist(signal_test_data[:,15],bins=100,label=r'Signal',histtype='step', linewidth=2, facecolor='none', edgecolor='orchid',fill=True,density=True)\n",
    "axs[1].set(xlabel=u'Leading muon $p_{T}$ (GeV)', ylabel='A.U')\n",
    "axs[1].semilogy()\n",
    "axs[1].legend(loc='best',frameon=False, ncol=1,fontsize='large')\n",
    "print(signal_test_data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the autoencoder\n",
    "\n",
    "Now, let's define an autoencoder to learn to reconstruct the training data after compressing it through a bottleneck, then decompressing it again.\n",
    "\n",
    "<img src=\"images/ae.png\" alt=\"The autoencoder\" width=\"800\" img align=\"center\"/>\n",
    "\n",
    "For that, we need a stack of dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:49:55.410969: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\n",
      "2021-12-02 11:49:55.411589: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 57)]              0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 57)               228       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                1856      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                64        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,536\n",
      "Trainable params: 5,230\n",
      "Non-trainable params: 306\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:49:56.552897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 11:49:56.553327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-02 11:49:56.553673: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\n",
      "2021-12-02 11:49:56.553750: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\n",
      "2021-12-02 11:49:56.553814: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\n",
      "2021-12-02 11:49:56.555760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\n",
      "2021-12-02 11:49:56.555839: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\n",
      "2021-12-02 11:49:56.555909: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\n",
      "2021-12-02 11:49:56.555923: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-02 11:49:56.556159: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "\n",
    "input_shape = 57\n",
    "latent_dim = 3\n",
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "x = BatchNormalization()(inputArray)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "#decoder\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "#create autoencoder\n",
    "autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "625/625 [==============================] - 3s 3ms/step - loss: 85.1087 - val_loss: 85.6662 - lr: 1.0000e-05\n",
      "Epoch 2/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 84.3443 - val_loss: 85.0049 - lr: 1.0000e-05\n",
      "Epoch 3/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 83.5592 - val_loss: 84.1634 - lr: 1.0000e-05\n",
      "Epoch 4/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 82.7759 - val_loss: 83.3677 - lr: 1.0000e-05\n",
      "Epoch 5/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 82.0019 - val_loss: 82.5765 - lr: 1.0000e-05\n",
      "Epoch 6/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 81.2384 - val_loss: 81.8416 - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 80.4968 - val_loss: 81.0473 - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 79.7643 - val_loss: 80.3202 - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 79.0417 - val_loss: 79.5827 - lr: 1.0000e-05\n",
      "Epoch 10/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 78.3176 - val_loss: 78.8264 - lr: 1.0000e-05\n",
      "Epoch 11/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 77.6000 - val_loss: 78.0947 - lr: 1.0000e-05\n",
      "Epoch 12/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 76.8670 - val_loss: 77.2725 - lr: 1.0000e-05\n",
      "Epoch 13/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 76.1180 - val_loss: 76.5922 - lr: 1.0000e-05\n",
      "Epoch 14/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 75.3596 - val_loss: 75.7776 - lr: 1.0000e-05\n",
      "Epoch 15/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 74.6091 - val_loss: 75.0573 - lr: 1.0000e-05\n",
      "Epoch 16/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 73.8535 - val_loss: 74.2918 - lr: 1.0000e-05\n",
      "Epoch 17/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 73.1199 - val_loss: 73.4201 - lr: 1.0000e-05\n",
      "Epoch 18/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 72.3898 - val_loss: 72.7608 - lr: 1.0000e-05\n",
      "Epoch 19/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 71.6778 - val_loss: 72.0798 - lr: 1.0000e-05\n",
      "Epoch 20/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 70.9647 - val_loss: 71.3264 - lr: 1.0000e-05\n",
      "Epoch 21/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 70.2646 - val_loss: 70.4917 - lr: 1.0000e-05\n",
      "Epoch 22/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 69.5660 - val_loss: 69.8217 - lr: 1.0000e-05\n",
      "Epoch 23/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 68.8795 - val_loss: 69.1408 - lr: 1.0000e-05\n",
      "Epoch 24/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 68.1979 - val_loss: 68.5706 - lr: 1.0000e-05\n",
      "Epoch 25/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 67.5107 - val_loss: 67.8443 - lr: 1.0000e-05\n",
      "Epoch 26/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 66.8315 - val_loss: 67.0935 - lr: 1.0000e-05\n",
      "Epoch 27/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 66.1580 - val_loss: 66.3780 - lr: 1.0000e-05\n",
      "Epoch 28/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 65.4899 - val_loss: 65.6694 - lr: 1.0000e-05\n",
      "Epoch 29/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 64.8286 - val_loss: 65.1187 - lr: 1.0000e-05\n",
      "Epoch 30/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 64.1768 - val_loss: 64.3303 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 63.5195 - val_loss: 63.6807 - lr: 1.0000e-05\n",
      "Epoch 32/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 62.8747 - val_loss: 63.0372 - lr: 1.0000e-05\n",
      "Epoch 33/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 62.2184 - val_loss: 62.5342 - lr: 1.0000e-05\n",
      "Epoch 34/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 61.5740 - val_loss: 61.8502 - lr: 1.0000e-05\n",
      "Epoch 35/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 60.9237 - val_loss: 61.1236 - lr: 1.0000e-05\n",
      "Epoch 36/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 60.2696 - val_loss: 60.5395 - lr: 1.0000e-05\n",
      "Epoch 37/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 59.6185 - val_loss: 59.8435 - lr: 1.0000e-05\n",
      "Epoch 38/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 58.9684 - val_loss: 59.1282 - lr: 1.0000e-05\n",
      "Epoch 39/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 58.3193 - val_loss: 58.5834 - lr: 1.0000e-05\n",
      "Epoch 40/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 57.6689 - val_loss: 57.7349 - lr: 1.0000e-05\n",
      "Epoch 41/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 57.0325 - val_loss: 57.2968 - lr: 1.0000e-05\n",
      "Epoch 42/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 56.3795 - val_loss: 56.5874 - lr: 1.0000e-05\n",
      "Epoch 43/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 55.7333 - val_loss: 55.8054 - lr: 1.0000e-05\n",
      "Epoch 44/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 55.0909 - val_loss: 55.2458 - lr: 1.0000e-05\n",
      "Epoch 45/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 54.4448 - val_loss: 54.6172 - lr: 1.0000e-05\n",
      "Epoch 46/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 53.7973 - val_loss: 53.9459 - lr: 1.0000e-05\n",
      "Epoch 47/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 53.1543 - val_loss: 53.3554 - lr: 1.0000e-05\n",
      "Epoch 48/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 52.5176 - val_loss: 52.7036 - lr: 1.0000e-05\n",
      "Epoch 49/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 51.8739 - val_loss: 52.0297 - lr: 1.0000e-05\n",
      "Epoch 50/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 51.2317 - val_loss: 51.4563 - lr: 1.0000e-05\n",
      "Epoch 51/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 50.5900 - val_loss: 50.6579 - lr: 1.0000e-05\n",
      "Epoch 52/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 49.9548 - val_loss: 49.9441 - lr: 1.0000e-05\n",
      "Epoch 53/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 49.3062 - val_loss: 49.3589 - lr: 1.0000e-05\n",
      "Epoch 54/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 48.6684 - val_loss: 48.8693 - lr: 1.0000e-05\n",
      "Epoch 55/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 48.0362 - val_loss: 48.2034 - lr: 1.0000e-05\n",
      "Epoch 56/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 47.3893 - val_loss: 47.5945 - lr: 1.0000e-05\n",
      "Epoch 57/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 46.7536 - val_loss: 46.7808 - lr: 1.0000e-05\n",
      "Epoch 58/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 46.1175 - val_loss: 46.2190 - lr: 1.0000e-05\n",
      "Epoch 59/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 45.4910 - val_loss: 45.5629 - lr: 1.0000e-05\n",
      "Epoch 60/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 44.8636 - val_loss: 44.8798 - lr: 1.0000e-05\n",
      "Epoch 61/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 44.2311 - val_loss: 44.3624 - lr: 1.0000e-05\n",
      "Epoch 62/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 43.6123 - val_loss: 43.7847 - lr: 1.0000e-05\n",
      "Epoch 63/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 42.9786 - val_loss: 43.0883 - lr: 1.0000e-05\n",
      "Epoch 64/150\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 42.3569 - val_loss: 42.4076 - lr: 1.0000e-05\n",
      "Epoch 65/150\n",
      " 43/625 [=>............................] - ETA: 1s - loss: 41.3554"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "\n",
    "callbacks=[]\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "if train:\n",
    "    history = autoencoder.fit(X_train, X_train, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_data=(X_val, X_val),\n",
    "                  callbacks=callbacks)\n",
    "    # Save the model\n",
    "    autoencoder.save('baseline_ae.h5')\n",
    "    \n",
    "else:\n",
    "    autoencoder = tf.keras.models.load_model('baseline_ae.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model performance\n",
    "\n",
    "Remember that the key metric we use for anomaly detection is the mean-squared-error: If the error is high, the data is more likely to be anomalous, and if the error is low, the data is similar to the training data (which in our case is SM events). We therefore first need to run `model.predict()` in order to get the AE reconstructed output, both for our vanilla SM test data, and for our new leptoquark signal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_prediction = autoencoder.predict(X_test)\n",
    "signal_prediction = autoencoder.predict(signal_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to compute the mean-square-error, which will be our final discriminating variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(true, prediction):\n",
    "    loss = tf.reduce_mean(tf.math.square(true - prediction),axis=-1)\n",
    "    return loss\n",
    "\n",
    "# compute loss value of input data versus AE reconstructed data\n",
    "mse_sm = mse_loss(X_test, bkg_prediction.astype(np.float32)).numpy()\n",
    "mse_bsm = mse_loss(signal_test_data,signal_prediction.astype(np.float32)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at our discriminant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size=100\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(mse_sm, bins=bin_size, label=\"SM Background\", density = True, histtype='step', fill=False, edgecolor='green', linewidth=1.5)\n",
    "plt.hist(mse_bsm, bins=bin_size, label=\"Leptoquark\", density = True, histtype='step', fill=False, edgecolor='orchid', linewidth=1.5)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Mean-squared-error\")\n",
    "plt.ylabel(\"Probability (a.u.)\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some discrimination power if we cut at very high values of the MSE! Let's look at a ROC curve to make it easier to vizualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "target_background = np.zeros(mse_sm.shape[0])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "trueVal = np.concatenate((np.ones(mse_bsm.shape[0]), target_background)) # anomaly=1, bkg=0\n",
    "predVal_loss = np.concatenate((mse_bsm, mse_sm))\n",
    "\n",
    "fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    \n",
    "plt.plot(fpr_loss, tpr_loss, \"-\", label='Leptoquark (auc = %.1f%%)'%(auc_loss*100.), linewidth=1.5, color = \"orchid\")\n",
    "    \n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend(loc='center right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='green', linestyle='dashed', linewidth=2) # threshold value for measuring anomaly detection efficiency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! So at a false positive rate of 10E-5, the signal efficiency is almost three orders of magnitude higher! This can obviously be further improved, but I leave that up to you :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model compression\n",
    "\n",
    "Now, there is absolutely no way anyone would let you deploy this model on an FPGA in the trigger. It will use far too many resources! Luckily, as we discussed in the lecture, there are some cheap tricks you can perform to compress the model. These are pruning and quantization-aware-training and both are very easily implemented. Let's have a look.\n",
    "\n",
    "To quantize the model during training, such that the network will get the opportunity to adapt to the narrower bitwidth we use the library [QKeras](https://www.nature.com/articles/s42256-021-00356-5.epdf?sharing_token=A6MQVmmncHNyCtDUXzrqtNRgN0jAjWel9jnR3ZoTv0N3uekY-CrHD1aJ9BTeJNRfQ1EhZ9jJIhgZjfrQxrmxMLMZ4eGzSeru7-ASFE-Xt3NVE6yorlffwUN0muAm1auU2I6-5ug4bOLCRYvA0mp-iT-OdPsrBYeH0IHRYx0t3wc%3D), developed in a joint effort between CERN and Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import QDense, QActivation\n",
    "\n",
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "x = BatchNormalization()(inputArray)\n",
    "x = QDense(32, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "x = QDense(16, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "encoder = QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(x)\n",
    "\n",
    "#decoder\n",
    "x = QDense(16, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(encoder)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "x = QDense(32, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = QActivation('quantized_relu(bits=8)')(x)\n",
    "decoder = QDense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "               kernel_quantizer='quantized_bits(8,0,1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(8,0,1, alpha=1.0)')(x)\n",
    "\n",
    "#create autoencoder\n",
    "q_autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "q_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy as that! Let's add some pruning on top, 50% sparsity (removing half of the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
    "model = prune.prune_low_magnitude(q_autoencoder, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss='mse')\n",
    "train = True\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "\n",
    "callbacks=[]\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "if train:\n",
    "    history = q_autoencoder.fit(X_train, X_train, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_data=(X_val, X_val),\n",
    "                  callbacks=callbacks)\n",
    "    # Save the model\n",
    "    q_autoencoder.save('compressed_ae.h5')\n",
    "    \n",
    "else:\n",
    "    q_autoencoder = tf.keras.models.load_model('compressed_ae.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the performance to the floating point precision, unpruned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_prediction = q_autoencoder.predict(X_test)\n",
    "signal_prediction = q_autoencoder.predict(signal_test_data)\n",
    "\n",
    "# compute loss value of input data versus AE reconstructed data\n",
    "q_mse_sm = mse_loss(X_test, bkg_prediction.astype(np.float32)).numpy()\n",
    "q_mse_bsm = mse_loss(signal_test_data,signal_prediction.astype(np.float32)).numpy()\n",
    "\n",
    "target_background = np.zeros(q_mse_sm.shape[0])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "trueVal = np.concatenate((np.ones(q_mse_bsm.shape[0]), target_background)) # anomaly=1, bkg=0\n",
    "predVal_loss = np.concatenate((q_mse_bsm, q_mse_sm))\n",
    "\n",
    "q_fpr_loss, q_tpr_loss, q_threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "q_auc_loss = auc(q_fpr_loss, q_tpr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "    \n",
    "plt.plot(fpr_loss, tpr_loss, \"-\", label='Baseline (auc = %.1f%%)'%(auc_loss*100.), linewidth=1.5, color = \"orchid\")\n",
    "plt.plot(q_fpr_loss, q_tpr_loss, \"-\", label='Quantized+Pruned (auc = %.1f%%)'%(q_auc_loss*100.), linewidth=1.5, color = \"green\")\n",
    "\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend(loc='center right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='orange', linestyle='dashed', linewidth=2) # threshold value for measuring anomaly detection efficiency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUCH! We really took a hit. Well, whatever. Let's tune that another time and nonw try deploying these models!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
