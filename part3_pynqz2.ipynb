{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment on the Pynq Z2 SoC\n",
    "\n",
    "Finally, we will deploy the two models and measure the inference latency on a Pynq Z2 System-on-Chip. You can buy this board yourself for under 200 euros!\n",
    "\n",
    "<img src=\"images/pynq.png\" alt=\"pynq\" width=\"500\" img align=\"center\"/>\n",
    "\n",
    "The first thing you have to do, is connect the board following [these instructions](https://pynq.readthedocs.io/en/latest/getting_started/pynq_z2_setup.html). I always connect it to a power source and then directly to my router, connect my computer to the WiFi on the same router, check which IP the pynq gets, then connect to the board in my browser with [http://<board IP address>](http://<board IP address>). You can also connect the board directly to your computer, but that honestly never worked for me on Mac. But please try by following the instructions on the link above! You will be prompted for a password, which is *xilinx*.\n",
    "\n",
    "You're in!\n",
    "\n",
    "Now, we need to copy over a few things by pressing the `Upload` button and find:\n",
    "- The two tarballs we made in the previous exercise \"baseline_ae_pynq_package.tar.gz\" and \"qkeras_ae_pynq_package.tar.gz\"\n",
    "- The part3_pynqz2.ipynb notebook\n",
    "\n",
    "That's it!\n",
    "\n",
    "Let's load our model onto the FPGA and check the inference latency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the bitfile, driver and some test data\n",
    "tar = tarfile.open(\"baseline_ae_pynq_package.tar.gz\")\n",
    "tar.extractall()\n",
    "\n",
    "driver = [f for f in os.listdir('./baseline_ae_pynq_package/') if 'driver' in f][0]\n",
    "shutil.copy(f'baseline_ae_pynq_package/{driver}', 'baseline_ae_pynq_package/hls4mlruntime.py')\n",
    "os.chdir('baseline_ae_pynq_package')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the hls4ml driver, it is super easy loading the network onto the Zynq Z2! We can also check the inference latency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hls4mlruntime import NeuralNetworkOverlay\n",
    "\n",
    "bitfile = [f for f in os.listdir() if '.bit' in f][0]\n",
    "\n",
    "X = np.load('X.npy').astype(np.float32)\n",
    "y_ref = np.load('y.npy').astype(np.float32)\n",
    "nn = NeuralNetworkOverlay(bitfile, X.shape, X.shape )\n",
    "y_hw, _, _ = nn.predict(X, X.shape, profile=True)\n",
    "np.testing.assert_allclose(y_hw, y_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! The total latency per inference is roughly 0.5s/55969 ~ 9 microseconds. However, this includes data transfer and software overhead! SO the \"real\" latency is still the one we saw from the reports (3 micorseconds).\n",
    "\n",
    "In the L1T, the data comes from optical fibers whearas in the Pynq its stored in memory, also we dont have the same software overhead in the L1-trigger.\n",
    "\n",
    "\n",
    "Let's verify that the latency is roughly the same for the quantized model also on the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the bitfile, driver and some test data\n",
    "os.chdir('../')  \n",
    "tar = tarfile.open(\"qkeras_ae_pynq_package.tar.gz\")\n",
    "tar.extractall()\n",
    "\n",
    "driver = [f for f in os.listdir('./qkeras_ae_pynq_package/') if 'driver' in f][0]\n",
    "shutil.copy(f'qkeras_ae_pynq_package/{driver}', 'qkeras_ae_pynq_package/hls4mlruntime.py')\n",
    "os.chdir('baseline_ae_pynq_package')   \n",
    "files = os.listdir('.')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitfile = [f for f in os.listdir() if '.bit' in f][0]\n",
    "\n",
    "X = np.load('X.npy').astype(np.float32)\n",
    "y_ref = np.load('y.npy').astype(np.float32)\n",
    "q_nn = NeuralNetworkOverlay(bitfile, X.shape, X.shape )\n",
    "y_hw, _, _ = q_nn.predict(X, X.shape, profile=True)\n",
    "np.testing.assert_allclose(y_hw, y_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let's also check out the true and reconstructed muon $p_{T}$ to convince ourselves that the model is on the board and doing... something? Note that we do not expected it to do something amazing, considering the latent dimension is 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(8,5))\n",
    "fig.suptitle('Kinematic distributions in test data versus reconstructed data')\n",
    "\n",
    "axs[0].hist(X[:,3],bins=100,label=r'Truth',histtype='step', linewidth=2, facecolor='none', edgecolor='green',fill=True,density=True)\n",
    "axs[0].hist(y_hw[:,3],bins=100,label=r'AE RECO',histtype='step', linewidth=2, facecolor='none', edgecolor='orchid',fill=True,density=True)\n",
    "# axs[0].semilogy()\n",
    "axs[0].set(xlabel=u'Leading electron $p_{T}$ (GeV)', ylabel='A.U')\n",
    "axs[0].legend(loc='best',frameon=False, ncol=1,fontsize='large')\n",
    "\n",
    "axs[1].hist(X[:,15],bins=100,label=r'Truth',histtype='step', linewidth=2, facecolor='none', edgecolor='green',fill=True,density=True)\n",
    "axs[1].hist(y_hw[:,15],bins=100,label=r'AE reco',histtype='step', linewidth=2, facecolor='none', edgecolor='orchid',fill=True,density=True)\n",
    "axs[1].set(xlabel=u'Leading muon $p_{T}$ (GeV)', ylabel='A.U')\n",
    "# axs[1].semilogy()\n",
    "axs[1].legend(loc='best',frameon=False, ncol=1,fontsize='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gross! This is definitely not a good generator. But we've also seen that the algorithms that reconstruct the data better, are not neccessarily the best anomaly detection algorithms. I'll leave it up to you to make it better :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
